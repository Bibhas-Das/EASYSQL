#RNN -> LSTM has 2 mechanigm memory cell,and gatting
'''
LSTM (Long sort term memeory) networks are prefered over simple RNNs (Recurrent)
Neural Networks) or traditional feeforwording neral networks (ANNs) for 
handelling textual datasets primarily because of their ability to effictively
capture long-term depenfencies and understand sequenstial patterns within
teh text.
'''
import os
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding,LSTM,Bidirectional,Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential
import re
import nltk
from tensorflow.keras.models import model_from_json
import pickle


def remove_emojies_and_special_chracters(sentence):
        sentence=re.sub(r'[^a-zA-Z0-9\s]','',sentence)
        return sentence.strip()


def preprocess_pipeline(data):
    sentences = data.split('\n')
    index=0
    for i in range(len(sentences)):
        sentences[index]=remove_emojies_and_special_chracters(sentences[i]).lower()
        if(len(sentences[index])>0):
            index+=1
    
    sentences=sentences[:index]


    tokenize=[]
    for sentence in sentences:
        sentence=sentence.lower()

    return sentences


def train_store():
    input_file="file.txt"

    with open(input_file,'r',encoding='utf-8') as file:
        data=file.read()

    os.system('cls')
    data=data[:50000]


    tokenize_sentences = preprocess_pipeline(data)

    tokenizer = Tokenizer(oov_token='<oov>')
    tokenizer.fit_on_texts(tokenize_sentences)

    #print(tokenizer.word_counts)
    #print(tokenizer.word_index)

    total_words = len(tokenizer.word_index)+1
    #print(total_words)

    #Sequence and N_GRAM make 
    input_sequences = []
    for line in tokenize_sentences:
        token_list = tokenizer.texts_to_sequences([line])[0]
        #print(token_list)
        for i in range(1,len(token_list)):
            n_gram_sequence = token_list[:i+1]
            input_sequences.append(n_gram_sequence)
    #print(input_sequences[:100])

    #add padding
    max_sequence_len = max([len(x) for x in input_sequences])
    input_sequences = np.array(pad_sequences(input_sequences,maxlen=max_sequence_len,padding='pre'))#pre/post   

    #print(input_sequences)# now they are queal length
    #print("#"*10)
    #print(input_sequences[:,:-1])

    X,label = input_sequences[:,:-1],input_sequences[:,-1]

    ys = tf.keras.utils.to_categorical(label,num_classes=total_words)

    #print(ys)

    #split data into tranning validation and test sets
    from sklearn.model_selection import train_test_split

    X_train_temp,X_val_test,Y_train_temp,Y_val_test = train_test_split(X,ys,test_size=0.2, random_state=42)
    X_val, X_test, Y_val, Y_test = train_test_split(X_val_test,Y_val_test,test_size=0.5, random_state=42) 

    model = Sequential()
    model.add(Embedding(total_words,100))
    model.add(Bidirectional(LSTM(150)))# 150 nerons
    model.add(Dense(total_words,activation='softmax'))

    adam = Adam(learning_rate=0.01)
    model.compile(loss='categorical_crossentropy',optimizer=adam, metrics=['accuracy'])
    # epochs: how many times trains
    history = model.fit(X_train_temp,Y_train_temp, epochs=50,validation_data=(X_val,Y_val),verbose=1)



    #save model achitecture as JSON

    # Save model architecture
    model_json = model.to_json()
    with open('lstm_model.json', 'w') as json_file:
        json_file.write(model_json)

    # Save model weights
    model.save_weights('lstm_model.weights.h5')  # Corrected filename


    with open("training_history.pkl", "wb") as file:
        pickle.dump(history.history, file)

    with open("tokenizer.pkl", "wb") as file:
        pickle.dump(tokenizer, file)

    with open('max_sequence_len.int','w') as file:
        file.write(str(max_sequence_len))


def Testing_plot():
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.metrics import confusion_matrix

    try:
        import pickle
        with open("training_history.pkl", "rb") as file:
            history = pickle.load(file)  # Load saved training history

        # Plot Training Loss
        plt.plot(history['loss'], label="Training Loss")
        plt.plot(history['val_loss'], label="Validation Loss")
        plt.title("Training and Validation Loss")
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.legend()
        plt.show()

        # Plot Training Accuracy
        plt.plot(history['accuracy'], label="Training Accuracy")
        plt.plot(history['val_accuracy'], label="Validation Accuracy")
        plt.title("Training and Validation Accuracy")
        plt.xlabel('Epochs')
        plt.ylabel('Accuracy')
        plt.legend()
        plt.show()

    except FileNotFoundError:
        print("Training history not found. Run training first to generate plots.")





def predict_top_5_words(model,tokenizer,seed_text):

    with open('max_sequence_len.int','r') as file:
        max_sequence_len=int(file.read())
    print("Value of max_sequence_len is stored successfully")

    token_list=tokenizer.texts_to_sequences([seed_text])[0]
    token_list = pad_sequences([token_list],maxlen=max_sequence_len-1,padding='pre')
    predicted = model.predict(token_list,verbose=0)
    top_five_indexes = np.argsort(predicted[0])[::-1][:5]
    top_five_words=[]
    for index in top_five_indexes:
        for word,indx in tokenizer.word_index.items():
            if indx == index:
                top_five_words.append(word)
                break
    return top_five_words






def main():
    # Load model architecture from JSON
    with open('lstm_model.json', 'r') as json_file:
        loaded_model_json = json_file.read()
    model = model_from_json(loaded_model_json)

    # Load weights into the model
    model.load_weights('lstm_model.weights.h5')  # Corrected filename

    # Compile the loaded model
    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])

    print("Model successfully loaded and ready for use!")



    # Load Tokenizer
    with open("tokenizer.pkl", "rb") as file:
        tokenizer = pickle.load(file)

    print("Tokenizer successfully loaded!")

    while True:
        seed_text =input("\nEnter text : ")
        print(seed_text," ",predict_top_5_words(model,tokenizer,seed_text))



train_store()
Testing_plot()
main()
